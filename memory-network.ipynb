{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-To-End Memory Networks\n",
    "\n",
    "#### Paper\n",
    "[https://arxiv.org/abs/1503.08895](https://arxiv.org/abs/1503.08895)\n",
    "\n",
    "#### Citation\n",
    "```\n",
    "@inproceedings{sukhbaatar2015end,\n",
    "  title={End-to-end memory networks},\n",
    "  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},\n",
    "  booktitle={Advances in neural information processing systems},\n",
    "  pages={2440--2448},\n",
    "  year={2015}\n",
    "}\n",
    "```\n",
    "\n",
    "#### Notes\n",
    "Unlike the paper, this implementation does not use any weight tying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import FloatTensor, LongTensor\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F, init\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "\n",
    "def Volatile(x):\n",
    "    return Variable(x, volatile=True)\n",
    "\n",
    "\n",
    "def add_grad_noise(parameters, scale=0.01):\n",
    "    for p in parameters:\n",
    "        p.grad.data.add_(torch.randn(p.size()).mul_(scale))\n",
    "\n",
    "\n",
    "def binary_cross_entropy_with_logits(input, target, weight=None, size_average=True):\n",
    "    \"\"\"Function that measures Binary Cross Entropy between target and output logits.\n",
    "    \n",
    "    From torch/nn/functional.py. In master, but not in v0.1.12.\n",
    "    \"\"\"\n",
    "    if not target.is_same_size(input):\n",
    "        raise ValueError('Target size ({}) must be the same as input size ({})'.format(target.size(), input.size()))\n",
    "\n",
    "    max_val = input.clamp(min=0)\n",
    "    loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    if size_average:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MemNet(nn.Module):\n",
    "    def __init__(self, input_size, query_size, hidden_size, n_layers=2):\n",
    "        super(MemNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.query_size = query_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.h = nn.Linear(query_size, hidden_size)\n",
    "        \n",
    "        self.fs = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_size) for i in range(n_layers)])\n",
    "        \n",
    "        self.gs = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_size) for i in range(n_layers)])\n",
    "    \n",
    "    def call1(self, x, q, return_masks=False):\n",
    "        masks = [] if return_masks else None\n",
    "        \n",
    "        z = self.h(q)\n",
    "        for f, g in zip(self.fs, self.gs):\n",
    "            u = f(x)\n",
    "            v = g(x)\n",
    "            p = F.softmax(z.mm(u.t()))\n",
    "            w = p.mm(v)\n",
    "            z = z + w\n",
    "\n",
    "            if return_masks:\n",
    "                masks.append(p)\n",
    "    \n",
    "        return (z, masks) if return_masks else z\n",
    "\n",
    "    def __call__(self, x, q, return_masks=False):\n",
    "        if x.ndimension() == 2:\n",
    "            return self.call1(x, q, return_masks=return_masks)\n",
    "        \n",
    "        masks = [] if return_masks else None\n",
    "\n",
    "        # Flatten inputs\n",
    "        batch_size, n_inputs, _ = x.size()\n",
    "        x_flat = x.view(batch_size*n_inputs, self.input_size)\n",
    "\n",
    "        # Flatten queries\n",
    "        batch_size2, n_queries, _ = q.size()\n",
    "        q_flat = q.view(batch_size2*n_queries, self.query_size)\n",
    "\n",
    "        # Batch sizes should match\n",
    "        assert batch_size == batch_size2\n",
    "\n",
    "        # z = f(q)\n",
    "        z_flat = self.h(q_flat)\n",
    "        z = z_flat.view(batch_size, n_queries, self.hidden_size)\n",
    "\n",
    "        for f, g in zip(self.fs, self.gs):\n",
    "            # u = f(x)\n",
    "            u_flat = f(x_flat)\n",
    "            u = u_flat.view(batch_size, n_inputs, self.hidden_size)\n",
    "            uT = u.transpose(1, 2)\n",
    "\n",
    "            # p = softmax(zu^T)\n",
    "            s = z.bmm(uT)\n",
    "            s_flat = s.view(batch_size*n_queries, n_inputs)\n",
    "            p_flat = F.softmax(s_flat)\n",
    "            p = p_flat.view(batch_size, n_queries, n_inputs)\n",
    "            \n",
    "            # v = g(x)\n",
    "            # w = pv\n",
    "            # z = z + w\n",
    "            v_flat = g(x_flat)\n",
    "            v = v_flat.view(batch_size, n_inputs, self.hidden_size)\n",
    "            w = p.bmm(v)\n",
    "            z = z + w\n",
    "            \n",
    "            if return_masks:\n",
    "                masks.append(p)\n",
    "\n",
    "        return (z, masks) if return_masks else z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 6\n",
    "n_queries = 5\n",
    "input_size = 10\n",
    "query_size = 9\n",
    "\n",
    "hidden_size = 4\n",
    "n_layers = 2\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "x = torch.randn(batch_size, n_inputs, input_size)\n",
    "q = torch.randn(batch_size, n_queries, query_size)\n",
    "y = (torch.rand(batch_size, n_queries) > 0.5).float()\n",
    "\n",
    "memnet = MemNet(input_size, query_size, hidden_size, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.2283  2.6824 -0.0098  0.3675\n",
      " 0.9428  0.9278 -0.3740 -0.0285\n",
      " 0.6457  0.2929 -0.4503  0.5719\n",
      " 0.0582  0.9509  0.5408  1.1107\n",
      "-0.7183  0.9618  0.1809 -1.0658\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# z = memnet(Volatile(x[0]), Volatile(q[0]))\n",
    "# print(z)\n",
    "i = 1\n",
    "z_i = memnet(Volatile(x[i]), Volatile(q[i]))\n",
    "print(z_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.2283  2.6824 -0.0098  0.3675\n",
      " 0.9428  0.9278 -0.3740 -0.0285\n",
      " 0.6457  0.2929 -0.4503  0.5719\n",
      " 0.0582  0.9509  0.5408  1.1107\n",
      "-0.7183  0.9618  0.1809 -1.0658\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n",
      "[Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1487  0.0907  0.1641  0.1450  0.0833  0.3682\n",
      "  0.0762  0.2533  0.0448  0.0899  0.5001  0.0356\n",
      "  0.2263  0.1909  0.1071  0.1175  0.3352  0.0230\n",
      "  0.1715  0.1430  0.1556  0.1380  0.1878  0.2041\n",
      "  0.1369  0.0571  0.1912  0.1224  0.0427  0.4497\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1799  0.2987  0.0106  0.0027  0.2075  0.3006\n",
      "  0.1921  0.2673  0.1263  0.0601  0.1619  0.1922\n",
      "  0.1812  0.2704  0.1383  0.0818  0.1310  0.1973\n",
      "  0.1543  0.0732  0.0291  0.0251  0.2994  0.4189\n",
      "  0.1366  0.0737  0.1424  0.2686  0.2131  0.1656\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.1641  0.1386  0.2658  0.1293  0.1235  0.1787\n",
      "  0.1747  0.1939  0.1723  0.1109  0.2143  0.1340\n",
      "  0.1572  0.2407  0.1473  0.1610  0.1112  0.1826\n",
      "  0.1872  0.2142  0.1419  0.0993  0.2557  0.1018\n",
      "  0.1628  0.2501  0.2024  0.1245  0.0804  0.1799\n",
      "[torch.FloatTensor of size 3x5x6]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.2535  0.0528  0.1127  0.0572  0.0373  0.4864\n",
      "  0.2386  0.0280  0.2889  0.0446  0.0297  0.3702\n",
      "  0.2238  0.1317  0.2368  0.1502  0.0817  0.1758\n",
      "  0.1224  0.1530  0.2546  0.2105  0.1243  0.1352\n",
      "  0.2147  0.1216  0.1277  0.1209  0.0916  0.3236\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1305  0.1419  0.3603  0.1536  0.0223  0.1915\n",
      "  0.1705  0.1938  0.2720  0.1386  0.0926  0.1324\n",
      "  0.1248  0.1512  0.3230  0.2114  0.0852  0.1045\n",
      "  0.0546  0.0657  0.3905  0.3820  0.0242  0.0830\n",
      "  0.1856  0.1503  0.0803  0.1163  0.1863  0.2812\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.0423  0.8166  0.0382  0.0231  0.0053  0.0744\n",
      "  0.1888  0.1306  0.1876  0.1827  0.1677  0.1427\n",
      "  0.1726  0.1499  0.2055  0.1561  0.1392  0.1767\n",
      "  0.2147  0.1175  0.1710  0.1618  0.1934  0.1415\n",
      "  0.1300  0.4498  0.1489  0.0813  0.0314  0.1586\n",
      "[torch.FloatTensor of size 3x5x6]\n",
      "]\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.4633  0.6788 -0.0766 -0.7502\n",
      "  0.9680  1.6115  0.5432 -0.6515\n",
      "  1.5941  0.5594 -0.2834 -0.0261\n",
      "  0.3418  0.6975  0.3369  0.5276\n",
      " -0.5941  0.3385  0.0235 -0.2853\n",
      "\n",
      "(1 ,.,.) = \n",
      "  1.2283  2.6824 -0.0098  0.3675\n",
      "  0.9428  0.9278 -0.3740 -0.0285\n",
      "  0.6457  0.2929 -0.4503  0.5719\n",
      "  0.0582  0.9509  0.5408  1.1107\n",
      " -0.7183  0.9618  0.1809 -1.0658\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.8803  0.6184 -0.1560 -1.4529\n",
      " -0.2949  0.5305 -0.0542 -0.0586\n",
      " -0.3003  0.1442 -0.2992 -0.2372\n",
      " -0.7972  1.0175  0.4901  0.2006\n",
      " -0.7178  0.3956 -0.5371 -0.7621\n",
      "[torch.FloatTensor of size 3x5x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z, p = memnet(Volatile(x), Volatile(q), return_masks=True)\n",
    "print(z[i])\n",
    "print(p)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = memnet(Volatile(x), Volatile(q))\n",
    "z_flat = z.view(batch_size*n_queries, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.7281\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = nn.Linear(hidden_size, 1)\n",
    "y_hat_flat = clf(z_flat)\n",
    "y_hat = y_hat_flat.view(batch_size, n_queries)\n",
    "loss = binary_cross_entropy_with_logits(y_hat, Volatile(y))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0762 -0.2696  0.5813  0.2067  0.0827\n",
      "-0.3050  0.3262  0.6307  0.1017 -0.3580\n",
      "-0.2444  0.1318  0.3082 -0.2508  0.0921\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "\n",
      " 0  1  1  0  0\n",
      " 1  1  0  1  1\n",
      " 0  0  1  1  0\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "\n",
      " 1  0  0  1  1\n",
      " 0  0  1  0  0\n",
      " 1  1  0  0  1\n",
      "[torch.ByteTensor of size 3x5]\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)\n",
    "print(y)\n",
    "print(y_hat > 0 == y)\n",
    "print((y_hat > 0 == y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
